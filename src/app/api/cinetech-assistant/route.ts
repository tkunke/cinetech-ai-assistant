import { NextRequest, NextResponse } from 'next/server';
import OpenAI, { APIError } from 'openai/index.mjs';
import fs from 'fs';
import { performBingSearch } from '@/utils/performBingSearch';
import { processSearchResults } from '@/utils/processSearchResults';
import { generateImage } from '@/utils/generateImage';
import { alternateImage } from '@/utils/alternateImage';
import { imageRecognition } from '@/utils/imageRecognition';
import { AssistantStream } from 'openai/lib/AssistantStream';

// Define the RunStatus type
interface RunStatus {
  status: string;
  required_action?: {
    submit_tool_outputs: {
      tool_calls: ToolCall[];
    };
  } | null; // Allow null for compatibility
  thread_id: string;
  id: string;
  failed?: boolean;
  usage?: RunUsage | null;
  engine?: string;
}

// Define the RunUsage interface
interface RunUsage {
  prompt_tokens: number;
  completion_tokens: number;
  total_tokens: number;
}

// Define the ToolCall type (ensure this matches your actual type)
interface ToolCall {
  id: string;
  function: {
    name: string;
    arguments: string;
  };
}

type ToolOutput = {
  tool_call_id: string;
  output: string | undefined;
  engine?: string;
};

type TextContent = {
  type: 'text';
  text: {
    value: string;
  };
};

type ImageContent = {
  type: 'image_file' | 'image_url';
  image: {
    url: string;
  };
};

type MessageContent = TextContent | ImageContent;

// Define the MessageCreateParams type
interface FileSearch {
  type: 'file_search';
}

interface Attachment {
  file_id: string;
  tools: FileSearch[] | any[]; // Allow different tool types
}

interface MessageCreateParams {
  role: 'user' | 'assistant';
  content: string;
  attachments?: Attachment[];
}

// In-memory store for run statuses
interface RunStatusStore {
  [key: string]: RunStatus; // Define the type of the store
}

const runStatusStore: RunStatusStore = {}; // Initialize the store with the proper type

interface ImageEngineStore {
  [key: string]: string | undefined;
}

const imageEngineStore: ImageEngineStore = {};

// In-memory store for temporarily stored files
interface FileStore {
  [key: string]: {
    filePath: string;
    fileType: string;
  };
}

interface APIError {
  status: number;
  error?: {
    message?: string;
  };
}

const fileStore: FileStore = {}; // Initialize the file store

const MAX_ATTEMPTS = 3;

// Define the event handler function
async function handleRunStatusEvent(event: { status: string, threadId: string, runId: string }) {
  const { status, threadId, runId } = event;
  const openai = new OpenAI();

  console.log(`Handling event for run status: ${status} for thread ID: ${threadId}`);

  if (status === 'completed') {
    console.log('Run is completed.');
    // Handle completion logic here
    return;
  }

  if (status === 'requires_action') {
    console.log('Action required. Processing tool calls.');
    const runStatus = await openai.beta.threads.runs.retrieve(threadId, runId);

    if (runStatus.required_action && runStatus.required_action.submit_tool_outputs) {
      const toolCalls = runStatus.required_action.submit_tool_outputs.tool_calls;
      const toolOutputs: ToolOutput[] = [];

      for (const toolCall of toolCalls) {
        const functionName = toolCall.function.name;
        const args = JSON.parse(toolCall.function.arguments);
        let output: string | undefined;
        let engine: string | undefined;
        let attemptCount = 0;

        while (attemptCount < MAX_ATTEMPTS) {
          try {
            if (functionName === 'performBingSearch') {
              const searchResults = await performBingSearch(args.user_request);
              const searchResultsString = JSON.stringify(searchResults);
              output = await processSearchResults(args.user_request, searchResultsString);
            } else if (functionName === 'generateImage') {
              const result = await generateImage(args.content);
              const { result: imageUrl, engine: resultEngine } = JSON.parse(result);
              output = `${imageUrl} (generated by ${resultEngine})`;
              engine = resultEngine;
              console.log(`Storing engine info in imageEngineStore: ${imageUrl} -> ${engine}`);
              imageEngineStore[imageUrl] = engine;
            } else if (functionName === 'alternateImage') {
              const result = await alternateImage(args.content);
              const { result: imageUrl, engine: resultEngine } = JSON.parse(result);
              output = `${imageUrl} (generated by ${resultEngine})`;
              engine = resultEngine;
              console.log(`Storing engine info in imageEngineStore: ${imageUrl} -> ${engine}`);
              imageEngineStore[imageUrl] = engine;
            } else if (functionName === 'imageRecognition') {
              const storedFile = fileStore[threadId];
              if (storedFile) {
                const { filePath, fileType } = storedFile;
                const imageRecognitionResponse = await imageRecognition(filePath, args.content);
                output = imageRecognitionResponse ?? undefined;
                fs.unlinkSync(filePath);
                delete fileStore[threadId];
              } else {
                console.error('No file provided for image recognition');
                output = undefined;
              }
            }

            if (output !== undefined) {
              const toolOutput: ToolOutput = { tool_call_id: toolCall.id, output: output };
              toolOutputs.push(toolOutput);
            }

            break;

          } catch (error) {
            attemptCount++;
            console.error(`Attempt ${attemptCount} failed for ${functionName}:`, error);
            if (attemptCount >= MAX_ATTEMPTS) {
              break;
            }
            await new Promise((resolve) => setTimeout(resolve, 1000));
          }
        }
      }

      if (toolOutputs.length > 0) {
        openai.beta.threads.runs.submitToolOutputsStream(threadId, runId, {
          tool_outputs: toolOutputs,
        });
        console.log('Submitting tool outputs: ', toolOutputs);
      }
    }
  }
}

// Simulate event subscription
async function simulateEventSubscription(threadId: string, runId: string) {
  const openai = new OpenAI();
  let finalRunStatus: RunStatus | null = null;

  // Simulate receiving events. Replace this with actual event subscription logic.
  while (true) {
    const runStatus = await openai.beta.threads.runs.retrieve(threadId, runId);
    await handleRunStatusEvent({
      status: runStatus.status,
      threadId: threadId,
      runId: runId,
    });

    if (runStatus.status === 'completed') {
      // Emit the thread.processing.completed event to the client
      //console.log('Emitting thread.processing.completed event');
      process.stdout.write(`event: thread.processing.completed\ndata: ${JSON.stringify({ threadId, runId })}\n\n`);
      runStatusStore[threadId] = { 
        status: 'completed', 
        thread_id: threadId, 
        id: runId,
        usage: runStatus.usage
      };
      console.log('runStatusStore updated: ', runStatus);
      finalRunStatus = runStatus;
      break;
    }

    await new Promise((resolve) => setTimeout(resolve, 1000)); // Simulate waiting for next event
  }

  // Ensure to store the final run status in memory or database so the client can fetch it
  if (finalRunStatus) {
    runStatusStore[threadId] = {
      status: 'completed',
      thread_id: threadId,
      id: runId,
      usage: finalRunStatus.usage
    };
  }
}

// Post a new message and stream OpenAI Assistant response
export async function POST(request: NextRequest) {
  console.log('POST request received');

  // Create OpenAI client
  const openai = new OpenAI();

  // Parse form data for file attachments
  const formData = await request.formData();
  const content = formData.get('content') as string;
  const assistantId = formData.get('assistantId') as string;
  let threadId = formData.get('threadId') as string | null;
  const file = formData.get('file') as File | null;

  // Define newMessage with proper typing
  let newMessage: MessageCreateParams = {
    role: 'user',
    content,
    attachments: [],
  };

  // Handle file upload if there's a file attached
  if (file) {
    const fileData = await file.arrayBuffer();
    const fileBuffer = Buffer.from(fileData);

    // Determine the appropriate tool based on the file type
    const fileExtension = file.name.split('.').pop()?.toLowerCase();
    if (fileExtension && ['png', 'jpeg', 'jpg', 'webp', 'gif'].includes(fileExtension)) {
      // Save the file temporarily to disk for later use
      const filePath = `/tmp/${file.name}`;
      fs.writeFileSync(filePath, fileBuffer);
      fileStore[threadId || ''] = { filePath, fileType: file.type };
    }
  }

  // If no thread id then create a new openai thread
  if (!threadId) {
    console.log('Creating a new thread');
    const thread = await openai.beta.threads.create();
    threadId = thread.id;
  }
  console.log('Thread ID:', threadId);

  // Add new message to thread
  console.log('Message to be added to thread:', JSON.stringify(newMessage, null, 2));
  try {
    await openai.beta.threads.messages.create(threadId, newMessage);
    console.log('Message added to thread', newMessage);

    // Create a run and stream it
    const runStream = openai.beta.threads.runs.stream(threadId, {
      assistant_id: assistantId,
    });

    const assistantStream = runStream; // Use runStream directly if it supports the necessary methods
    const readableStream = runStream.toReadableStream();
    console.log('Readable stream created');

    // Get the initial runId from the assistantStream
    let initialRunId: string | undefined;
    while (!initialRunId) {
      const runStatus = assistantStream.currentRun();
      if (runStatus) {
        initialRunId = runStatus.id;
      } else {
        console.log('Initial run is undefined, retrying...');
        await new Promise(resolve => setTimeout(resolve, 1000));
      }
    }

    // Call the simulateEventSubscription function with the threadId and initialRunId
    simulateEventSubscription(threadId, initialRunId).then(() => {
      console.log('Event subscription simulation completed.');
    }).catch((error) => {
      console.error('Error during event subscription simulation:', error);
    });

    return new Response(readableStream);

  } catch (error) {
    console.error('Error adding message to thread:', error);
    const apiError = error as APIError; // Type assertion
    if (apiError.status === 400 && apiError?.error?.message?.includes("Can't add messages to thread")) {
      runStatusStore[threadId] = { status: 'failed', thread_id: threadId, id: '', failed: true };
    }
    return new Response('Failed to add message to thread', { status: 500 });
  }
}

// Get all of the OpenAI Assistant messages associated with a thread
export async function GET(request: NextRequest) {
  const searchParams = request.nextUrl.searchParams;
  const threadId = searchParams.get('threadId');
  const type = searchParams.get('type');

  if (type === 'engineInfo') {
    const imageUrl = searchParams.get('imageUrl');
    console.log(`Fetching engine info for imageUrl: ${imageUrl}`); // Debugging line
    if (imageUrl && imageEngineStore[imageUrl]) {
      console.log(`Engine found: ${imageEngineStore[imageUrl]}`); // Debugging line
      return NextResponse.json({ engine: imageEngineStore[imageUrl] });
    } else {
      console.log('No engine info found'); // Debugging line
      return NextResponse.json({ engine: null });
    }
  }

  if (threadId == null) {
    throw new Error('Missing threadId');
  }

  const openai = new OpenAI();

  try {
    const threadMessages = await openai.beta.threads.messages.list(threadId);
    const cleanMessages = threadMessages.data.map((m) => {
      let content = '';
      if (m.content && Array.isArray(m.content) && m.content.length > 0) {
        const messageContent = m.content[0] as MessageContent;
        if (messageContent.type === 'text') {
          content = messageContent.text.value;
        } else if (messageContent.type === 'image_file' || messageContent.type === 'image_url') {
          content = messageContent.image.url;
        }
      }
      return {
        id: m.id,
        role: m.role,
        content: content,
        createdAt: m.created_at,
      };
    });

    cleanMessages.reverse();
    const runStatus = runStatusStore[threadId];

    if (runStatus && (runStatus.status === 'completed' || runStatus.failed)) {
      const tokenUsage = {
        prompt_tokens: runStatus.usage?.prompt_tokens ?? 0,
        completion_tokens: runStatus.usage?.completion_tokens ?? 0,
        total_tokens: runStatus.usage?.total_tokens ?? 0,
      };

      console.log('Message usage:', tokenUsage);
      return NextResponse.json({
        messages: cleanMessages,
        messageCount: cleanMessages.length,
        runStatus: runStatus,
        tokenUsage: tokenUsage,
      });
    }
    console.log('server message count: ', cleanMessages.length);

    return NextResponse.json({
      messages: cleanMessages,
      messageCount: cleanMessages.length,
      runStatus: runStatus,
    });
  } catch (error) {
    console.error('Error fetching messages and run status:', error);
    throw new Error('Failed to fetch messages and run status');
  }
}
